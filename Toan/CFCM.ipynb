{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import davies_bouldin_score, silhouette_score, calinski_harabasz_score\n",
    "from scipy.spatial.distance import cdist\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_membership(n_samples, n_clusters):\n",
    "    u = np.random.rand(n_samples, n_clusters)\n",
    "    return u / np.sum(u, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centroids(data, membership, m):\n",
    "    um = membership ** m\n",
    "    return (um.T @ data) / np.sum(um.T, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_membership(data, centroids, m):\n",
    "    distances = cdist(data, centroids, metric='euclidean')\n",
    "    distances = np.fmax(distances, 1e-10)\n",
    "    inv_distances = 1.0 / distances\n",
    "    exponent = 2.0 / (m - 1)\n",
    "    return (inv_distances / np.sum(inv_distances[:, :, None] ** exponent, axis=1)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_u_bar(labels, n_clusters, supervised_ratio):\n",
    "    n_samples = len(labels)\n",
    "    supervised_count = int(n_samples * supervised_ratio)\n",
    "    supervised_indices = np.random.choice(n_samples, supervised_count, replace=False)\n",
    "    \n",
    "    u_bar = np.zeros((n_samples, n_clusters))\n",
    "    le = LabelEncoder()\n",
    "    int_labels = le.fit_transform(labels)\n",
    "    u_bar[supervised_indices, int_labels[supervised_indices]] = 1.0\n",
    "    \n",
    "    return u_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cfcm_clustering(data, labels, n_clusters, m=2, epsilon=1e-5, max_iter=300, supervised_ratio=0.3):\n",
    "    n_samples = data.shape[0]\n",
    "    membership_matrix = initialize_membership(n_samples, n_clusters)\n",
    "    u_bar = create_u_bar(labels, n_clusters, supervised_ratio)\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        prev_membership = membership_matrix.copy()\n",
    "        centroids = compute_centroids(data, membership_matrix - u_bar, m)\n",
    "        membership_matrix = update_membership(data, centroids, m)\n",
    "        membership_matrix = u_bar + (1 - np.sum(u_bar, axis=1, keepdims=True)) * membership_matrix\n",
    "        \n",
    "        if np.linalg.norm(membership_matrix - prev_membership) < epsilon:\n",
    "            break\n",
    "    \n",
    "    return centroids, membership_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_coefficient(U):\n",
    "    return np.sum(U**2) / U.shape[0]\n",
    "\n",
    "def classification_entropy(U):\n",
    "    return -np.sum(U * np.log(U + 1e-10)) / U.shape[0]\n",
    "\n",
    "def separation(X, U, V, M):\n",
    "    distances = np.linalg.norm(X[:, np.newaxis, :] - V, axis=2) ** 2  # Shape (n_samples, n_clusters)\n",
    "    weighted_distances = (U**M) * distances  # Cùng shape (n_samples, n_clusters)\n",
    "    return np.sum(weighted_distances)  # Tổng khoảng cách có trọng số\n",
    "\n",
    "\n",
    "def hypervolume(U, M):\n",
    "    return np.sum(U**M)\n",
    "\n",
    "def cs(X, U, V, M):\n",
    "    return np.sum(U**M * np.linalg.norm(X[:, np.newaxis, :] - V, axis=2) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def evaluate_and_print_results(title, X, U, V, process_time, step):\n",
    "    M = 2  # Hệ số fuzziness\n",
    "\n",
    "    # Làm tròn giá trị hiển thị\n",
    "    def wdvl(val):\n",
    "        return str(round(val, 4))\n",
    "\n",
    "    labels_pred = np.argmax(U, axis=1)\n",
    "\n",
    "    results = [\n",
    "        title.ljust(8),                      # Tên thuật toán\n",
    "        wdvl(process_time).ljust(6),         # Thời gian chạy\n",
    "        str(step).ljust(6),                  # Số bước lặp\n",
    "        wdvl(davies_bouldin_score(X, labels_pred)).ljust(6),  # DB index\n",
    "        wdvl(partition_coefficient(U)).ljust(6),  # PC\n",
    "        wdvl(classification_entropy(U)).ljust(6),  # CE\n",
    "        wdvl(separation(X, U, V, M)).ljust(8),  # S\n",
    "        wdvl(calinski_harabasz_score(X, labels_pred)).ljust(10),  # CH\n",
    "        wdvl(silhouette_score(X, labels_pred)).ljust(6),  # SI\n",
    "        wdvl(hypervolume(U, M)).ljust(6),  # FHV\n",
    "        wdvl(cs(X, U, V, M)).ljust(6),  # CS\n",
    "    ]\n",
    "    \n",
    "    print(\"  \".join(results))  # In theo cột\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alg      Time   Step   DB-    PC-    CE-    S-      CH+        SI+    FHV+   CS-\n",
      "--------------------------------------------------------------------------------\n",
      "CFCM      0.0841  5       1.2422  0.3753  8.7195  911.2534  2.8809      -0.3225  66.4267  911.2534\n"
     ]
    }
   ],
   "source": [
    "# In tiêu đề bảng\n",
    "print(\"Alg      Time   Step   DB-    PC-    CE-    S-      CH+        SI+    FHV+   CS-\")\n",
    "print(\"-\" * 80)\n",
    "datasets = [ \"D:/Truong_NCKH/Data/Iris.csv\", \"D:/Truong_NCKH/Data/Wine.csv\"]\n",
    "# Chạy thuật toán và in kết quả\n",
    "for dataset in datasets:\n",
    "    df = pd.read_csv(dataset)\n",
    "    labels = df.iloc[:, -1].values\n",
    "    data = StandardScaler().fit_transform(df.iloc[:, :-1].values)\n",
    "    \n",
    "    n_clusters = len(np.unique(labels))\n",
    "    \n",
    "    start_time = time.perf_counter()  \n",
    "    centroids, membership_matrix = cfcm_clustering(data, labels, n_clusters)\n",
    "    process_time = time.perf_counter() - start_time  \n",
    "\n",
    "    evaluate_and_print_results(\"CFCM\", data, membership_matrix, centroids, process_time=process_time, step=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
